#!/usr/bin/env -S python -u

# This script takes .feather files generated by UrbanPop, including both person details and daytime-nightime assignments, and
# converts them into a csv file with the information that is needed for ExaEpi.
# This script also generates a c++ header file containing the structure of the data, including functions for reading and writing
# the data.

import sys
import os.path
import os
import pandas
from pandas.api.types import CategoricalDtype
import numpy as np
import time
import argparse
import geopandas


# only include these fields in the output csv and c++ structure

include_fields = [
    'p_id',
    'h_id',
    'geoid',
    'pr_age',
    'pr_sex',
    'pr_race',
    'pr_travel',
    'pr_veh_occ',
    'role',
    'naics',
    'grade',
    'school_id',
    ]

PUMS_ID_LEN = 14
NAICS_LEN = 9

# note: missing category indexes are written as -1
# we could extract these from the dataset, but then they will not be in a suitable order, even with sorting
# so we predefine and check for values in the data that are not listed here
categ_types = {
    'hh_type':
        CategoricalDtype(categories=["hh", "gq"]),
    'hh_living_arrangement':
        CategoricalDtype(categories=["married", "male_no_spouse", "female_no_spouse", "alone", "not_alone"]),
    'hh_has_kids':
        CategoricalDtype(categories=["no", "yes"]),
    'hh_dwg':
        CategoricalDtype(categories=[
            "single_fam_detach", "single_fam_attach", "2_unit", "3_4_unit", "5_9_unit", "10_19_unit", "20_49_unit", "GE50_unit",
            "mob_home", "other"]),
    'hh_tenure':
        CategoricalDtype(categories=["own", "rent", "other"]),
    'hh_vehicles':
        CategoricalDtype(categories=["01", "02", "03", "04", "05", "GE06"]),
    'pr_sex':
        CategoricalDtype(categories=["female", "male"]),
    'pr_race':
        CategoricalDtype(categories=["white", "blk_af_amer", "asian", "native_amer", "pac_island", "other", "mult"]),
    'pr_hsplat':
        CategoricalDtype(categories=["no", "yes"]),
    'pr_ipr':
        CategoricalDtype(categories=["L050", "050_099", "100_124", "125_149", "150_184", "185_199", "GE200"]),
    'pr_emp_stat':
        CategoricalDtype(categories=["not.in.force", "unemp", "employed", "mil"]),
    'pr_travel':
        CategoricalDtype(
            categories=["car_truck_van", "public_transportation", "bicycle", "walked", "motorcycle", "taxicab", "other", "wfh"]),
    'pr_veh_occ':
        CategoricalDtype(categories=["drove_alone", "carpooled"]),
    'pr_grade':
        CategoricalDtype(categories=[
            "childcare",
            "preschl", "kind", "1st", "2nd", "3rd", "4th", "5th", "6th", "7th", "8th", "9th", "10th", "11th", "12th",
            "undergrad", "grad"]),
    'role':
        CategoricalDtype(categories=['nope', 'worker', 'student']),
    'naics_2010':
        CategoricalDtype(categories=[
           'edu_med_sca', # 0 Educational services, and health care and social assistance
           'con',         # 1 Construction
           'prf',         # 2 Professional, scientific, and management, and administrative, and waste management services
           'agr_ext',     # 3 Agriculture, forestry, fishing and hunting, and mining
           'mfg',         # 4 Manufacturing
           'wfh',         # 5 Work from home
           'ent',         # 6 Arts, entertainment, and recreation, and accommodation and food services
           'fin',         # 7 Finance and insurance, and real estate, and rental and leasing
           'ret',         # 8 Retail trade
           'srv',         # 9 Other services, except public administration
           'adm',         # 10 Public administration
           'utl_trn',     # 11 Transportation and warehousing, and utilities
           'whl',         # 12 Wholesale trade
           'inf'          # 13 Information
           ]),
    'naics':   # 2017 - latest
        CategoricalDtype(categories=[
           'agr_ffh',     # 0 Agriculture, forestry, fishing and hunting
           'ext',         # 1 Mining, quarrying, and oil and gas extraction
           'utl',         # 2 Utilities
           'con',         # 3 Construction
           'mfg',         # 4 Manufacturing
           'whl',         # 5 Wholesale trade
           'ret',         # 6 Retail trade
           'trn_whs',     # 7 Transportation and warehousing
           'inf',         # 8 Information
           'fin_ins',     # 9 Finance and insurance
           'rrl',         # 10 Real estate rental and leasing
           'prf',         # 11 Professional, scientific and technical services
           'mgt',         # 12 Management of companies and enterprises
           'adm_wmr',     # 13 Administrative and support and waste management and remediating services
           'edu',         # 14 Educational services
           'med_sca',     # 15 Health care and social services
           'ent',         # 16 Arts, entertainment and recreation
           'afs',         # 17 Accomodation and food services
           'srv',         # 18 Other services (except public administration)
           'pad',         # 19 Public administration
           'wfh',         # 20 Work from home
           ]),
    'grade':
        CategoricalDtype(categories=[
            'childcare',
            'k12pub_preschl', 'k12pub_kind', 'k12pub_1st', 'k12pub_2nd', 'k12pub_3rd', 'k12pub_4th', 'k12pub_5th', 'k12pub_6th',
            'k12pub_7th', 'k12pub_8th', 'k12pub_9th', 'k12pub_10th', 'k12pub_11th', 'k12pub_12th',
            'k12pv_preschl', 'k12pv_kind', 'k12pv_1st', 'k12pv_2nd', 'k12pv_3rd', 'k12pv_4th', 'k12pv_5th', 'k12pv_6th',
            'k12pv_7th', 'k12pv_8th', 'k12pv_9th', 'k12pv_10th', 'k12pv_11th', 'k12pv_12th',
            'undergrad', 'grad'])
    # public elementary 2-7, middle 8-10, high 11-14
    # private elementary 16-21, middle 22-24, high 25-28
    # college 29-30 (original data has 29-32, undergrade_female, undergrad_male, grad_female, grad_male
}

NAICS_EDU = 14
NAICS_WFH = 20


def print_header(df):
    string_fields = {"pums_id": "PUMS_ID_LEN", "pr_naics": "NAICS_LEN"}
    hdr_fname = "UrbanPopAgentStruct.H"
    print("Writing C++ header file at", hdr_fname)

    hdr = f"""
/*! @file {hdr_fname}
    \\brief Contains #UrbanPopAgent class used for reading in UrbanPop data
    File automatically generated by UrbanPop-scripts/extract_urbanpop_feather.py
*/

#pragma once

#include <stdlib.h>
#include <string.h>
#include <fstream>
#include <sstream>

using std::string;
using float32_t = float;

namespace UrbanPop {{

const size_t NUM_COLS = {len(df.columns)};

"""
    #const size_t PUMS_ID_LEN = {PUMS_ID_LEN};
    #const size_t NAICS_LEN = {NAICS_LEN};
    #"""

    # print out string arrays with category names
    for field_type in df:
        if field_type in categ_types:
            categs_expected = list(categ_types[field_type].categories)
            num_categs = len(categs_expected)
            hdr += f"""const int {field_type.upper()}_COUNT = {num_categs};\n"""
            hdr += f"""static string {field_type}_descriptions[{field_type.upper()}_COUNT] = {{"{'", "'.join(categs_expected)}"}};\n"""
            hdr += f"""struct {field_type.upper()} {{ enum {{{', '.join(categs_expected)}}}; }};\n"""
    hdr += "\n"

    hdr += """
static std::vector<string> split_string(const string &s, char delim) {
    std::vector<string> elems;
    std::stringstream ss(s);
    string item;
    while (std::getline(ss, item, delim)) elems.push_back(item);
    return elems;
}

"""

    hdr += 'struct UrbanPopAgent {\n'
    for i, col in enumerate(df.columns):
        if col in string_fields:
            hdr += f"""    char {col}[{string_fields[col]}];\n"""
        else:
            hdr += f"""    {df.dtypes.iloc[i]}_t {col};\n"""

    hdr += """
    bool read_csv(std::ifstream &f) {
        string buf;
        if (!getline(f, buf)) return false;
        if (buf[0] != '*') {
            id = -1;
            return true;
        }
        try {
            std::vector<string> tokens = split_string(buf.substr(2), ',');
            if (tokens.size() != NUM_COLS)
                throw std::runtime_error("Incorrect number of tokens, expected " + std::to_string(NUM_COLS) +
                                         " got " + std::to_string(tokens.size()));\n"""

    for i, col in enumerate(df.columns):
        if col in string_fields:
            hdr += f"""            AMREX_ALWAYS_ASSERT(!tokens[{i}].empty());\n"""
            hdr += f"""            strncpy({col}, tokens[{i}].c_str(), {string_fields[col]});\n"""
        else:
            if df.dtypes.iloc[i] == "float32":
                hdr += f"""            {col} = stof(tokens[{i}]);\n"""
            elif df.dtypes.iloc[i] == "int64":
                hdr += f"""            {col} = stol(tokens[{i}]);\n"""
            elif df.dtypes.iloc[i] == "string":
                hdr += f"""            {col} = tokens[{i});\n"""
            else:
                hdr += f"""            {col} = stoi(tokens[{i}]);\n"""
    hdr += """
        } catch (const std::exception &ex) {
            std::ostringstream os;
            os << "Error reading UrbanPop input file: " << ex.what() << ", line read: " << "'" << buf << "'";
            amrex::Abort(os.str());
        }
        return true;
    }\n"""

    hdr += """
    friend std::ostream& operator<<(std::ostream& os, const UrbanPopAgent& agent) {
        os << std::fixed << std::setprecision(6);\n"""

    for i, col in enumerate(df.columns):
        c_type = str(df.dtypes.iloc[i]) + "_t"
        if col in string_fields:
            hdr += '        os << string(agent.' + col + ', ' + string_fields[col] + ')'
        elif col in categ_types:
            hdr += '        os << (int)agent.' + col + ' << (agent.' + col + ' != -1 ? ":" + ' + col + '_descriptions[agent.' + \
                   col + '] : "")'
        else:
            hdr += "        os << " + ("(int)" if c_type == "int8_t" else "") + "agent." + col

        if i < len(df.columns) - 1:
            hdr += ' << ",";\n'
        else:
            hdr += ";"
        #os << (int)agent.hh_dwg << (agent.hh_dwg != -1 ? ":" + hh_dwg_descriptions[agent.hh_dwg] : "") << ",";

    hdr += """
        return os;
    }
};
} // namespace UrbanPop
"""

    f_hdr = open(hdr_fname, "w")
    print(hdr, file=f_hdr)
    f_hdr.close()
    print("Wrote", len(df.columns), "fields to", hdr_fname)


def process_nt_dt_feather_files(fnames, out_fname):
    start_t = time.time()
    dfs = []
    for i, fname in enumerate(fnames):
        print("Reading data from", fname, end=': ')
        t = time.time()
        df_metro = pandas.read_feather(fname)
        dfs.append(df_metro)
        print(len(dfs[-1].index), "records in %.3f s" % (time.time() - t))

    df = pandas.concat(dfs)
    df.sort_values(by=['p_id'], inplace=True)
    df.orig_geoid = df.orig_geoid.astype("int64")
    df.dest_geoid = df.dest_geoid.astype("int64")

    # convert school ids to unique 64 bit ints
    unique_school_ids = sorted(df.school_id.unique(), key=lambda x: (x is None, x))
    school_id_map = dict(zip(unique_school_ids, range(len(unique_school_ids))));
    school_id_map[None] = -1

    df["school_name"] = df["school_id"]
    df["school_id"] = df["school_id"].map(school_id_map).apply(lambda x: x).astype("int64")
    # make the valid school ids start at 1, 0 means no school
    df['school_id'] += 1
    # correct students to have only undergrad and grad, not split by male/female
    df['grade'] = df['grade'].replace(['undergrad_female'], 'undergrad', regex=True)
    df['grade'] = df['grade'].replace(['undergrad_male'], 'undergrad', regex=True)
    df['grade'] = df['grade'].replace(['grad_female'], 'grad', regex=True)
    df['grade'] = df['grade'].replace(['grad_male'], 'grad', regex=True)

    print("Processed", len(df.index), "records in %.3f s" % (time.time() - start_t))

    t = time.time()
    df.to_csv(out_fname + ".work.csv", sep=' ', index=False)
    print("Wrote", out_fname + ".work.csv in %.3f s" % (time.time() - t))

    return df


def assign_educators_to_school(required_educators, df, school, school_geoid, min_grade, max_grade, local_geoid_only):
    required_educators -= len(df.loc[(df['role'] == 1) & (df['school_id'] == school) & (df['work_geoid'] == school_geoid)])
    if required_educators < 0:
        print("ERROR: negative required educators", required_educators)
        sys.exit(0)
    if required_educators == 0:
        return 0
    if local_geoid_only:
        # select from the same work geoid
        df_educators = df.loc[(df['role'] == 1) & (df['naics'] == NAICS_EDU) & (df['school_id'] == 0) &
                              (df['work_geoid'] == school_geoid)]
    else:
        # select the same county (first 5 of GEOID)
        factor = 10**7
        county_code = school_geoid / factor - school_geoid % factor / factor
        df_educators = df.loc[(df['role'] == 1) & (df['naics'] == NAICS_EDU) & (df['school_id'] == 0) &
                              (df['work_geoid'] / factor - df['work_geoid'] % factor / factor == county_code)]

    if len(df_educators) == 0:
        return required_educators

    if len(df_educators) <= required_educators:
        educator_sample = df_educators
    else:
        educator_sample = df_educators.sample(n=required_educators)

    df.loc[df['id'].isin(educator_sample['id']), ['school_id', 'work_geoid']] = [school, school_geoid]
    # randomly choosing the grade is ok if the students are spread out equally across the grades, which we'd generally
    # expect, except for colleges, where there are far more undergrads than grads. So that will need a special case
    # FIXME: special case for colleges
    df.loc[df['id'].isin(educator_sample['id']), 'grade'] = \
        np.random.randint(min_grade, max_grade + 1, size=len(educator_sample)).astype('int8')
    return required_educators - len(educator_sample)


def allocate_educators(df, out_fname):
    df_special = df[['id', 'age', 'role', 'grade', 'school_id']]
    df_special = df_special.loc[(df_special['school_id'] != '') & (df_special['school_id'] != 0)]
    t = time.time()
    df_special.to_csv(out_fname + ".students.csv", sep=' ', index=False)
    print("Wrote", out_fname + ".students.csv in %.3f s" % (time.time() - t))
    print("Student population", len(df_special))

    t = time.time()
    # allocate educators at a ratio of 15:1 - this is the average for the US
    student_schools_map = {}
    schools = df.school_id.unique()
    for _, school in enumerate(schools):
        if school == 0:
            continue
        students_df = df.loc[(df['school_id'] == school) & (df['role'] == 2)]
        max_grade = students_df.grade.max()
        min_grade = students_df.grade.min()
        if min_grade < 0:
            print("ERROR: min grade is", min_grade, max_grade, "for school", school, students_df.grade)
            sys.exit(0)
        geoids = students_df.work_geoid.unique()
        if len(geoids) != 1:
            print("WARNING: more than one unique geoids for the school")
            sys.exit(0)
        student_schools_map[school] = [len(students_df.index), geoids[0], min_grade, max_grade]
    print("Counted students for", len(schools), "schools in %.3f s" % (time.time() - t))

    df_edu = df.loc[(df['role'] == 1) & (df['naics'] == NAICS_EDU)]
    print("NAICS edu population", len(df_edu))

    t = time.time()
    with open(out_fname + ".schools.csv", mode='w') as f:
        print("school students geoid min_grade max_grade", file=f)
        for _, school in enumerate(schools):
            if school == -1:
                continue
            if not school in student_schools_map:
                continue
            student_pop, school_geoid, min_grade, max_grade = student_schools_map[school]
            print(school, student_pop, school_geoid, min_grade, max_grade, file=f)

    print("Wrote", len(schools), "schools to", out_fname + ".schools.csv", "in %.3f s" % (time.time() - t))

    t = time.time()
    shortfall_educators = 0
    shortfall_educator_schools = 0
    # first allocate educators from local geoids
    for _, school in enumerate(schools):
        if school == -1:
            continue
        if not school in student_schools_map:
            continue
        student_pop, school_geoid, min_grade, max_grade = student_schools_map[school]
        required_educators = int(student_pop / 15)
        if required_educators == 0:
            continue
        required_educators = assign_educators_to_school(required_educators, df, school, school_geoid, min_grade, max_grade, True)
        if required_educators > 0:
            shortfall_educators += required_educators
            shortfall_educator_schools += 1
    print("Local educator shortfall:", shortfall_educator_schools, "schools;", shortfall_educators, "educators")

    shortfall_educators = 0
    shortfall_educator_schools = 0
    # now allocate educators from the county
    for _, school in enumerate(schools):
        if school == -1:
            continue
        if not school in student_schools_map:
            continue
        student_pop, school_geoid, min_grade, max_grade = student_schools_map[school]
        required_educators = int(student_pop / 15)
        if required_educators == 0:
            continue
        required_educators = assign_educators_to_school(required_educators, df, school, school_geoid, min_grade, max_grade, False)
        if required_educators > 0:
            shortfall_educators += required_educators
            shortfall_educator_schools += 1
    print("Final educator shortfall:", shortfall_educator_schools, "schools;", shortfall_educators, "educators")

    df_educators = df.loc[(df['school_id'] != 0) & (df['role'] == 1)]
    print("Allocated", len(df_educators), "educators in %.3f s" % (time.time() - t))

    t = time.time()
    df_educators.to_csv(out_fname + ".educators.csv", sep=' ', index=False)
    print("Wrote", out_fname + ".educators.csv in %.3f s" % (time.time() - t))



def process_feather_files(fnames, out_fname, geoid_locs_map, df_dt_nt, long_ids):
    global PUMS_ID_LEN
    global NAICS_LEN

    start_t = time.time()
    dfs = []
    for fname in fnames:
        print("Reading data from", fname, end=': ')
        t = time.time()
        df_read = pandas.read_feather(fname)
        df_read.to_csv(fname + ".csv")
        dfs.append(df_read)
        print(len(dfs[-1].index), "records in %.3f s" % (time.time() - t))

    df = pandas.concat(dfs)
    print("Processed", len(df.index), "records in %.3f s" % (time.time() - start_t))

    df.geoid = df.geoid.astype("int64")
    df.h_id = df.h_id.str.split("-").str[-1].astype("int32")

    # need to sort to ensure the order is the same between the population files and the daytime/nighttime files
    df.sort_values(by=['p_id'], inplace=True)
    # shouldn't need to sort this one
    #df_dt_nt.sort_values(by=['p_id'], inplace=True)
    if not np.array_equal(df.p_id.values, df_dt_nt.p_id.values):
        print("Mismatched p_ids for population vs daytime/nightime")
        sys.exit(0)
    if not np.array_equal(df.geoid.values, df_dt_nt.orig_geoid.values):
        print("Mismatched geoids for population vs daytime/nightime")
        sys.exit(0)

    # remove all not found in include list
    cols_to_purge = set(list(df.columns.values)) - set(include_fields)
    df.drop(columns=cols_to_purge, inplace=True)

    if "hh_income" in include_fields:
        # pack structure by moving int32_t value to before all int8_t values
        df.insert(4, "hh_income", df.pop("hh_income"))

    if "pums_id" in include_fields:
        # set specific ID types
        PUMS_ID_LEN = df.pums_id.map(len).max()
        print("Unique PUMS", len(df.pums_id.unique()), "max length", PUMS_ID_LEN)
        # move char arrays to end of struct
        df.insert(len(df.columns) - 1, "pums_id", df.pop("pums_id"))
    if "pr_naics" in include_fields:
        NAICS_LEN = df.pr_naics.map(len).max()
        print("Unique NAICS", len(df.pr_naics.unique()), "max length", NAICS_LEN)
        df.insert(len(df.columns) - 1, "pr_naics", df.pop("pr_naics"))
        # ensure the NAICS fields don't contain an empty string, which can muddle parsing down the line
        #df['pr_naics'] = df['pr_naics'].replace(["^\s*$"], 'NA', regex=True)
        df['pr_naics'] = df['pr_naics'].replace([''], 'NA', regex=True)

    df.rename(columns={"geoid": "home_geoid"}, inplace=True)
    df.rename(columns={"h_id": "household_id"}, inplace=True)

    # add lat/long locations from geoids
    df.insert(df.columns.get_loc("home_geoid") + 1, "home_lat", float(0))
    df.home_lat = df.home_lat.astype("float32")
    df.insert(df.columns.get_loc("home_lat") + 1, "home_lng", float(0))
    df.home_lng = df.home_lng.astype("float32")

    df.insert(df.columns.get_loc("home_lng") + 1, "work_geoid", int(0))
    df.insert(df.columns.get_loc("work_geoid") + 1, "work_lat", float(0))
    df.work_lat = df.work_lat.astype("float32")
    df.insert(df.columns.get_loc("work_lat") + 1, "work_lng", float(0))
    df.work_lng = df.work_lng.astype("float32")

    # get values from worker data
    df["work_geoid"] = df_dt_nt["dest_geoid"].values
    for col in ["role", "naics", "grade", "school_id"]:
        df[col] = df_dt_nt[col].values
        if not np.array_equal(df[col].values, df_dt_nt[col].values):
            print("Mismatched", col, "for population vs daytime/nightime")
            sys.exit(0)

    if not np.array_equal(df.work_geoid.values, df_dt_nt.dest_geoid.values):
        print("Mismatched work geoids for population vs daytime/nightime")
        sys.exit(0)

    for field_type in df:
        if field_type in categ_types:
            # compare the list with the unique to make sure we haven't fonud anything not in our predefined list
            categs_found = list(df[field_type].unique())
            categs_expected = list(categ_types[field_type].categories)
            missing = [x for x in categs_found if x not in categs_expected and x != '' and x is not None]
            if missing:
                print("WARNING: Found missing categories for", field_type, ":", missing)
            df[field_type] = df[field_type].astype(categ_types[field_type]).cat.codes
        else:
            if df.dtypes[field_type] == object:
                df[field_type] = df[field_type].astype("string")
            else:
                # this converts all float to int
                max_val = df[field_type].max()
                if max_val < 128:
                    df[field_type] = df[field_type].astype("int8")
                elif max_val < 2**15:
                    df[field_type] = df[field_type].astype("int16")
                elif max_val < 2**31:
                    df[field_type] = df[field_type].astype("int32")
                else:
                    df[field_type] = df[field_type].astype("int64")

    t = time.time()

    print("Setting lat/long for data", end=" ", flush=True)
    # find lat/long for each row entry
    df["home_lat"] = df["home_geoid"].map(geoid_locs_map).apply(lambda x: x[0]).astype("float32")
    df["home_lng"] = df["home_geoid"].map(geoid_locs_map).apply(lambda x: x[1]).astype("float32")

    df["work_lat"] = df["work_geoid"].map(geoid_locs_map).apply(lambda x: x[0]).astype("float32")
    df["work_lng"] = df["work_geoid"].map(geoid_locs_map).apply(lambda x: x[1]).astype("float32")
    print("\nSet lat/long for", len(df.index), "agents in %.3f s" % (time.time() - t))

    #print("Sorting by geoid")
    print("Sorting by lat/lng")
    t = time.time()
    df.sort_values(by=["home_lat", "home_lng"], inplace=True)
    print("Sorted in %.3f s" % (time.time() - t))
    out_fname_csv = out_fname + ".csv"
    out_fname_idx = out_fname + ".idx"
    print("Writing CSV text data to", out_fname_csv, "and block group indexes to", out_fname_idx)
    t = time.time()
    num_rows = len(df.index)
    # make sure all the p_ids are globally unique and just integers
    if not long_ids:
        df['p_id'] = np.arange(0, num_rows)

    df.rename(columns={"p_id": "id"}, inplace=True)
    for col in df.columns:
        if col.startswith("pr_") and col != 'pr_naics':
            df.rename(columns={col: col[3:]}, inplace=True)
        elif col.startswith("h_"):
            df.rename(columns={col: "home_" + col[2:]}, inplace=True)
        elif col.startswith("w_"):
            df.rename(columns={col: "work_" + col[2:]}, inplace=True)
        elif col.startswith("hh_"):
            df.rename(columns={col: "household_" + col[3:]}, inplace=True)

    print("Fields are:\n", df.dtypes, sep="")

    print_header(df)

    allocate_educators(df, out_fname)
    # need to reset the work lat/lng after potentially changing the work geoids for educators
    df["work_lat"] = df["work_geoid"].map(geoid_locs_map).apply(lambda x: x[0]).astype("float32")
    df["work_lng"] = df["work_geoid"].map(geoid_locs_map).apply(lambda x: x[1]).astype("float32")

    t = time.time()
    naics_types = list(categ_types['naics'].categories)
    num_naics = len(naics_types)
    work_geoids_map = {}
    work_geoids = df.work_geoid.unique()
    num_workers = 0
    for i, geoid in enumerate(work_geoids):
        # don't include wfh
        subset_df = df.loc[(df['work_geoid'] == geoid) & (df['role'] == 1) & (df['naics'] != NAICS_WFH)]
        naics_counts = []
        for naics_i in range(num_naics):
            naics_counts.append(len(subset_df.loc[subset_df['naics'] == naics_i]))
        work_pops = [len(subset_df.index)]
        if work_pops[0] != sum(naics_counts):
            print("ERROR: NAICS codes don't sum up to work population for geoid", geoid, work_pops[0], sum(naics_counts))
            sys.exit(0)
        num_workers += work_pops[0]
        work_pops.extend(naics_counts)
        work_geoids_map[geoid] = work_pops
    print("workers population", num_workers)
    print("Found", len(df.home_geoid.unique()), "home locations and", len(work_geoids_map),
          "work locations in %.3f s" % (time.time() - t))

    # start with a distinct marker so that the file can be read in parallel more easily
    df.index = ['*'] * num_rows

    # set school ids to be unique only to work geoid
    work_geoids = df.work_geoid.unique()
    for geoid in work_geoids:
        subset_df = df.loc[(df['work_geoid'] == geoid) & (df['school_id'] != 0)]
        if len(subset_df) == 0:
            continue
        # set school id to be unique only to work geoid
        min_school_id = subset_df['school_id'].min() - 1
        df.loc[(df['work_geoid'] == geoid) & (df['school_id'] != 0), 'school_id'] -= min_school_id

    # print each geoid in turn so we can track the file offsets
    with open(out_fname_idx, mode='w') as f:
        print("geoid lat lng foff h_pop w_pop", ' '.join(naics_types), file=f)
        geoids = df.home_geoid.unique()
        work_geoids = df.work_geoid.unique()
        if len(work_geoids) > len(geoids):
            only_work_geoids = list(set(work_geoids) - set(geoids))
            print("Work-only geoids", only_work_geoids)
            for i, geoid in enumerate(only_work_geoids):
                work_pops = work_geoids_map[geoid] if geoid in work_geoids_map else []
                print(geoid, ' '.join(map(str, geoid_locs_map[geoid])), 0, 0, ' '.join(map(str, work_pops)), file=f)

        for i, geoid in enumerate(geoids):
            foffset = os.stat(out_fname_csv).st_size if i > 0 else 0
            subset_df = df.loc[df['home_geoid'] == geoid]
            min_hh_id = subset_df['household_id'].min()
            # set household id to be unique only to home geoid
            subset_df.loc[:, 'household_id'] -= min_hh_id
            subset_df.to_csv(out_fname_csv, index=True, header=(i == 0), mode='w' if i == 0 else 'a', float_format="%.6f")
            work_pops = work_geoids_map[geoid] if geoid in work_geoids_map else []
            print(geoid,
                  ' '.join(map(str, geoid_locs_map[geoid])),
                  foffset,
                  len(subset_df.index),
                  ' '.join(map(str, work_pops)),
                  file=f)
    print("Wrote", len(df.index), "records in %.3f s" % (time.time() - t))

    fips_codes = []
    for i, geoid in enumerate(geoids):
        fips_codes.append(int(str(geoid)[:5]))
    print("FIPS codes:", sorted(set(fips_codes)))


def process_census_bg_shape_file(dir_names, geoid_locs_map):
    for dname in dir_names:
        # remove trailing slash if it exists
        if dname[-1] == "/":
            dname = dname[:-1]
        shape_fname = dname + "/" + os.path.split(dname)[1] + ".shp"
        # don't actually need to compute the centroid because the block group file has it under the INTPTLAT10 and INTPTLON10 cols
        #df = geopandas.read_file(shape_fname, include_fields=["GEOID10", "geometry"])
        #df = df.to_crs(crs=4326)
        #df["centroid"] = df.centroid
        print("Reading shape files at", shape_fname)
        df = geopandas.read_file(shape_fname, include_fields=["GEOID10", "INTPTLAT10", "INTPTLON10"], ignore_geometry=True)
        df.GEOID10 = df.GEOID10.astype("int64")
        df.INTPTLAT10 = df.INTPTLAT10.astype("float32")
        df.INTPTLON10 = df.INTPTLON10.astype("float32")
        df.to_csv(shape_fname + ".csv")
        print("Wrote", len(df.index), "GEOID locations to", shape_fname + ".csv")
        #print(df.dtypes)
        geoid_locs_map.update(df.set_index("GEOID10").T.to_dict("list"))


if __name__ == "__main__":
    t = time.time()
    np.random.seed(29)
    parser = argparse.ArgumentParser(description="Convert UrbanPop feather files to C++ struct binary file")
    parser.add_argument("--output", "-o", required=True, help="Output file")
    parser.add_argument("--files", "-f", required=True, nargs="+", help="Feather files")
    parser.add_argument("--shape_files_dir", "-s", required=True, nargs="+",
                        help="Directories for census block group shape files. Available from\n" + \
                        "https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2010&layergroup=Block+Groups")
    parser.add_argument("--day_night_files",
                        "-d",
                        required=True,
                        nargs="+",
                        help="Feather files containing daytime and nighttime locations")
    parser.add_argument("--long_ids", "-l", action="store_true", help="Use original long UrbanPop ids for agents")
    args = parser.parse_args()

    geoid_locs_map = {}
    process_census_bg_shape_file(args.shape_files_dir, geoid_locs_map)
    print("GEOID to locations map contains", len(geoid_locs_map), "entries")

    df_dt_nt = process_nt_dt_feather_files(args.day_night_files, args.output)

    process_feather_files(args.files, args.output, geoid_locs_map, df_dt_nt, args.long_ids)

    print("Processed", len(args.files), "files in %.2f s" % (time.time() - t))
