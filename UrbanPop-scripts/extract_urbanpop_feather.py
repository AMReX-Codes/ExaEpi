#!/usr/bin/env python

# This script takes .feather files generated by UrbanPop, including both person details and daytime-nightime assignments, and
# converts them into a csv file with the information that is needed for ExaEpi.
# This script also generates a c++ header file containing the structure of the data, including functions for reading and writing
# the data.

import sys
import os.path
import os
import pandas
from pandas.api.types import CategoricalDtype
import numpy as np
import time
import argparse
import geopandas
import scipy
import scipy.stats


# only include these fields in the output csv and c++ structure

include_fields = [
    'p_id',
    'hh_id',
    'geoid',
    'pr_age',
    'pr_sex',
    'pr_race',
    'pr_travel',
    'pr_veh_occ',
    'role',
    'naics',
    'grade',
    'school_id'
    ]

PUMS_ID_LEN = 14
NAICS_LEN = 9

# note: missing category indexes are written as -1
# we could extract these from the dataset, but then they will not be in a suitable order, even with sorting
# so we predefine and check for values in the data that are not listed here
categ_types = {
    'hh_type':
        CategoricalDtype(categories=["hh", "gq"]),
    'hh_living_arrangement':
        CategoricalDtype(categories=["married", "male_no_spouse", "female_no_spouse", "alone", "not_alone"]),
    'hh_has_kids':
        CategoricalDtype(categories=["no", "yes"]),
    'hh_dwg':
        CategoricalDtype(categories=[
            "single_fam_detach", "single_fam_attach", "2_unit", "3_4_unit", "5_9_unit", "10_19_unit", "20_49_unit", "GE50_unit",
            "mob_home", "other"]),
    'hh_tenure':
        CategoricalDtype(categories=["own", "rent", "other"]),
    'hh_vehicles':
        CategoricalDtype(categories=["01", "02", "03", "04", "05", "GE06"]),
    'pr_sex':
        CategoricalDtype(categories=["female", "male"]),
    'pr_race':
        CategoricalDtype(categories=["white", "blk_af_amer", "asian", "native_amer", "pac_island", "other", "mult"]),
    'pr_hsplat':
        CategoricalDtype(categories=["no", "yes"]),
    'pr_ipr':
        CategoricalDtype(categories=["L050", "050_099", "100_124", "125_149", "150_184", "185_199", "GE200"]),
    'pr_emp_stat':
        CategoricalDtype(categories=["not.in.force", "unemp", "employed", "mil"]),
    'pr_travel':
        CategoricalDtype(
            categories=["car_truck_van", "public_transportation", "bicycle", "walked", "motorcycle", "taxicab", "other", "wfh"]),
    'pr_veh_occ':
        CategoricalDtype(categories=["drove_alone", "carpooled"]),
    'pr_grade':
        CategoricalDtype(categories=[
            "preschl", "kind", "1st", "2nd", "3rd", "4th", "5th", "6th", "7th", "8th", "9th", "10th", "11th", "12th",
            "undergrad", "grad"]),
    'role':
        CategoricalDtype(categories=['nope', 'worker', 'student']),
    'naics':
        CategoricalDtype(categories=[
           'edu_med_sca', 'con', 'prf', 'agr_ext', 'mfg', 'wfh', 'ent', 'fin', 'ret', 'srv', 'adm', 'utl_trn', 'whl', 'inf']),
    'grade':
        CategoricalDtype(categories=[
            'childcare',
            'k12pub_preschl', 'k12pub_kind', 'k12pub_1st', 'k12pub_2nd', 'k12pub_3rd', 'k12pub_4th', 'k12pub_5th', 'k12pub_6th',
            'k12pub_7th', 'k12pub_8th', 'k12pub_9th', 'k12pub_10th', 'k12pub_11th', 'k12pub_12th',
            'k12pv_preschl', 'k12pv_kind', 'k12pv_1st', 'k12pv_2nd', 'k12pv_3rd', 'k12pv_4th', 'k12pv_5th', 'k12pv_6th',
            'k12pv_7th', 'k12pv_8th', 'k12pv_9th', 'k12pv_10th', 'k12pv_11th', 'k12pv_12th',
            'undergrad_female', 'undergrad_male', 'grad_female', 'grad_male'])
}



def print_header(df):
    string_fields = {"pums_id": "PUMS_ID_LEN", "pr_naics": "NAICS_LEN"}
    hdr_fname = "UrbanPopAgentStruct.H"
    print("Writing C++ header file at", hdr_fname)

    hdr = f"""
/*! @file {hdr_fname}
    \\brief Contains #UrbanPopAgent class used for reading in UrbanPop data
    File automatically generated by UrbanPop-scripts/extract_urbanpop_feather.py
*/

#pragma once

#include <stdlib.h>
#include <string.h>
#include <fstream>
#include <sstream>

using std::string;
using float32_t = float;

"""
#const size_t PUMS_ID_LEN = {PUMS_ID_LEN};
#const size_t NAICS_LEN = {NAICS_LEN};
#const size_t NUM_COLS = {len(df.columns)};
#"""

    # print out string arrays with category names
    for field_type in df:
        if field_type in categ_types:
            categs_expected = list(categ_types[field_type].categories)
            hdr += f"""static string {field_type}_descriptions[] = {{"{'", "'.join(categs_expected)}"}};\n"""
    hdr += "\n"

    hdr += """
static std::vector<string> split_string(const string &s, char delim) {
    std::vector<string> elems;
    std::stringstream ss(s);
    string item;
    while (std::getline(ss, item, delim)) elems.push_back(item);
    return elems;
}

"""

    hdr += 'struct UrbanPopAgent {\n'
    for i, col in enumerate(df.columns):
        if col in string_fields:
            hdr += f"""    char {col}[{string_fields[col]}];\n"""
        else:
            hdr += f"""    {df.dtypes.iloc[i]}_t {col};\n"""

    hdr += """
    bool read_csv(std::ifstream &f) {
        string buf;
        if (!getline(f, buf)) return false;
        if (buf[0] != '*') {
            p_id = -1;
            return true;
        }
        try {
            std::vector<string> tokens = split_string(buf.substr(2), ',');
            if (tokens.size() != NUM_COLS)
                throw std::runtime_error("Incorrect number of tokens, expected " + std::to_string(NUM_COLS) +
                                         " got " + std::to_string(tokens.size()));\n"""

    for i, col in enumerate(df.columns):
        if col in string_fields:
            hdr += f"""            AMREX_ALWAYS_ASSERT(!tokens[{i}].empty());\n"""
            hdr += f"""            strncpy({col}, tokens[{i}].c_str(), {string_fields[col]});\n"""
        else:
            if df.dtypes.iloc[i] == "float32":
                hdr += f"""            {col} = stof(tokens[{i}]);\n"""
            elif df.dtypes.iloc[i] == "int64":
                hdr += f"""            {col} = stol(tokens[{i}]);\n"""
            elif df.dtypes.iloc[i] == "string":
                hdr += f"""            {col} = tokens[{i});\n"""
            else:
                hdr += f"""            {col} = stoi(tokens[{i}]);\n"""
    hdr += """
        } catch (const std::exception &ex) {
            std::ostringstream os;
            os << "Error reading UrbanPop input file: " << ex.what() << ", line read: " << "'" << buf << "'";
            amrex::Abort(os.str());
        }
        return true;
    }\n"""

    hdr += """
    friend std::ostream& operator<<(std::ostream& os, const UrbanPopAgent& agent) {\n"""

    for i, col in enumerate(df.columns):
        c_type = str(df.dtypes.iloc[i]) + "_t"
        if col in string_fields:
            hdr += '        os << string(agent.' + col + ', ' + string_fields[col] + ') << ",";\n'
        elif col in categ_types:
            hdr += '        os << (int)agent.' + col + ' << (agent.' + col + ' != -1 ? ":" + ' + col + '_descriptions[agent.' + \
                   col + '] : "") << ",";\n'
        else:
            hdr += "        os << " + ("(int)" if c_type == "int8_t" else "") + "agent." + col + " << ',';\n"

        #os << (int)agent.hh_dwg << (agent.hh_dwg != -1 ? ":" + hh_dwg_descriptions[agent.hh_dwg] : "") << ",";

    hdr += """
        return os;
    }
};
"""

    f_hdr = open(hdr_fname, "w")
    print(hdr, file=f_hdr)
    f_hdr.close()
    print("Wrote", len(df.columns), "fields to", hdr_fname)


def process_nt_dt_feather_files(fnames):
    start_t = time.time()
    dfs = []
    for fname in fnames:
        print("Reading data from", fname, end=': ')
        t = time.time()
        dfs.append(pandas.read_feather(fname))
        print(len(dfs[-1].index), "records in %.3f s" % (time.time() - t))

    df = pandas.concat(dfs)

    df.orig_geoid = df.orig_geoid.astype("int64")
    df.dest_geoid = df.dest_geoid.astype("int64")

    # convert school ids to unique 64 bit ints
    unique_school_ids = sorted(df.school_id.unique(), key=lambda x: (x is None, x))
    school_id_map = dict(zip(unique_school_ids, range(len(unique_school_ids))));
    school_id_map[None] = -1

    df["school_id"] = df["school_id"].map(school_id_map).apply(lambda x: x).astype("int64")

    print("Processed", len(dfs[-1].index), "records in %.3f s" % (time.time() - start_t))

    return df


def process_feather_files(fnames, out_fname, geoid_locs_map, df_dt_nt):
    global PUMS_ID_LEN
    global NAICS_LEN

    dfs = []
    for fname in fnames:
        print("Reading data from", fname, end=': ')
        t = time.time()
        dfs.append(pandas.read_feather(fname))
        print(len(dfs[-1].index), "records in %.3f s" % (time.time() - t))

    df = pandas.concat(dfs)
    df.geoid = df.geoid.astype("int64")
    df.h_id = df.h_id.str.split("-").str[-1].astype("int32")

    # need to sort to ensure the order is the same between the population files and the daytime/nighttime files
    df.sort_values(by=['p_id'], inplace=True)
    # shouldn't need to sort this one
    #df_dt_nt.sort_values(by=['p_id'], inplace=True)
    if not np.array_equal(df.p_id.values, df_dt_nt.p_id.values):
        print("Mismatched p_ids for population vs daytime/nightime")
        sys.exit(0)
    if not np.array_equal(df.geoid.values, df_dt_nt.orig_geoid.values):
        print("Mismatched geoids for population vs daytime/nightime")
        sys.exit(0)

    # remove all not found in include list
    cols_to_purge = set(list(df.columns.values)) - set(include_fields)
    df.drop(columns=cols_to_purge, inplace=True)

    if "hh_income" in include_fields:
        # pack structure by moving int32_t value to before all int8_t values
        df.insert(4, "hh_income", df.pop("hh_income"))

    if "pums_id" in include_fields:
        # set specific ID types
        PUMS_ID_LEN = df.pums_id.map(len).max()
        print("Unique PUMS", len(df.pums_id.unique()), "max length", PUMS_ID_LEN)
        # move char arrays to end of struct
        df.insert(len(df.columns) - 1, "pums_id", df.pop("pums_id"))
    if "pr_naics" in include_fields:
        NAICS_LEN = df.pr_naics.map(len).max()
        print("Unique NAICS", len(df.pr_naics.unique()), "max length", NAICS_LEN)
        df.insert(len(df.columns) - 1, "pr_naics", df.pop("pr_naics"))
        # ensure the NAICS fields don't contain an empty string, which can muddle parsing down the line
        #df['pr_naics'] = df['pr_naics'].replace(["^\s*$"], 'NA', regex=True)
        df['pr_naics'] = df['pr_naics'].replace([''], 'NA', regex=True)

    df.rename(columns={"geoid": "home_geoid"}, inplace=True)
    df.rename(columns={"h_id": "household_id"}, inplace=True)

    # add lat/long locations from geoids
    df.insert(df.columns.get_loc("home_geoid") + 1, "home_lat", float(0))
    df.home_lat = df.home_lat.astype("float32")
    df.insert(df.columns.get_loc("home_lat") + 1, "home_lng", float(0))
    df.home_lng = df.home_lng.astype("float32")

    df.insert(df.columns.get_loc("home_lng") + 1, "work_geoid", int(0))
    df.insert(df.columns.get_loc("work_geoid") + 1, "work_lat", float(0))
    df.work_lat = df.work_lat.astype("float32")
    df.insert(df.columns.get_loc("work_lat") + 1, "work_lng", float(0))
    df.work_lng = df.work_lng.astype("float32")

    df["work_geoid"] = df_dt_nt["dest_geoid"].copy(deep=True)
    for col in ["role", "naics", "grade", "school_id"]:
        df[col] = df_dt_nt[col].copy(deep=True)

    for field_type in df:
        if field_type not in include_fields:
            continue
        if field_type in categ_types:
            # compare the list with the unique to make sure we haven't fonud anything not in our predefined list
            categs_found = list(df[field_type].unique())
            categs_expected = list(categ_types[field_type].categories)
            missing = [x for x in categs_found if x not in categs_expected and x != '' and x is not None]
            if missing:
                print("WARNING: Found missing categories for", field_type, ":", missing)
            df[field_type] = df[field_type].astype(categ_types[field_type]).cat.codes
        else:
            if df.dtypes[field_type] == object:
                df[field_type] = df[field_type].astype("string")
            else:
                # this converts all float to int
                max_val = df[field_type].max()
                if max_val < 128:
                    df[field_type] = df[field_type].astype("int8")
                elif max_val < 2**15:
                    df[field_type] = df[field_type].astype("int16")
                elif max_val < 2**31:
                    df[field_type] = df[field_type].astype("int32")
                else:
                    df[field_type] = df[field_type].astype("int64")

    t = time.time()

    print("Setting lat/long for data", end=" ", flush=True)
    # find lat/long for each row entry
    df["home_lat"] = df["home_geoid"].map(geoid_locs_map).apply(lambda x: x[0]).astype("float32")
    df["home_lng"] = df["home_geoid"].map(geoid_locs_map).apply(lambda x: x[1]).astype("float32")

    df["work_lat"] = df["work_geoid"].map(geoid_locs_map).apply(lambda x: x[0]).astype("float32")
    df["work_lng"] = df["work_geoid"].map(geoid_locs_map).apply(lambda x: x[1]).astype("float32")
    print("\nSet lat/long for", len(df.index), "agents in %.3f s" % (time.time() - t))

    #print("Sorting by geoid")
    print("Sorting by lat/lng")
    t = time.time()
    df.sort_values(by=["home_lat", "home_lng"], inplace=True)
    print("Sorted in %.3f s" % (time.time() - t))
    out_fname_csv = out_fname + ".csv"
    out_fname_idx = out_fname + ".idx"
    print("Writing CSV text data to", out_fname_csv, "and block group indexes to", out_fname_idx)
    t = time.time()
    num_rows = len(df.index)
    # make sure all the p_ids are globally unique (they are only unique to each urbanpop feather file originally)
    df['p_id'] = np.arange(0, num_rows)
    # start with a distinct marker so that the file can be read in parallel more easily
    df.index = ['*'] * num_rows

    df.rename(columns={"p_id": "id"}, inplace=True)
    for col in df.columns:
        if col.startswith("pr_"):
            df.rename(columns={col: col[3:]}, inplace=True)
        elif col.startswith("h_"):
            df.rename(columns={col: "home_" + col[2:]}, inplace=True)
        elif col.startswith("w_"):
            df.rename(columns={col: "work_" + col[2:]}, inplace=True)
        elif col.startswith("hh_"):
            df.rename(columns={col: "household_" + col[3:]}, inplace=True)

    print("Fields are:\n", df.dtypes, sep="")

    print_header(df)

    work_geoids_map = {}
    work_geoids = df.work_geoid.unique()
    for i, geoid in enumerate(work_geoids):
        subset_df = df.loc[df['work_geoid'] == geoid]
        work_geoids_map[geoid] = len(subset_df.index)
    print("Found", len(df.home_geoid.unique()), "home locations and", len(work_geoids_map), "work locations")

    # print each geoid in turn so we can track the file offsets
    with open(out_fname_idx, mode='w') as f:
        print("geoid lat lng foff h_pop w_pop", file=f)
        geoids = df.home_geoid.unique()
        for i, geoid in enumerate(geoids):
            foffset = os.stat(out_fname_csv).st_size if i > 0 else 0
            subset_df = df.loc[df['home_geoid'] == geoid]
            subset_df.to_csv(out_fname_csv, index=True, header=(i == 0), mode='w' if i == 0 else 'a')
            work_pop = work_geoids_map[geoid] if geoid in work_geoids_map else 0
            print(geoid, ' '.join(map(str, geoid_locs_map[geoid])), foffset, len(subset_df.index), work_pop, file=f)
    print("Wrote", len(df.index), "records in %.3f s" % (time.time() - t))


def process_census_bg_shape_file(dir_names, geoid_locs_map):
    for dname in dir_names:
        # remove trailing slash if it exists
        if dname[-1] == "/":
            dname = dname[:-1]
        shape_fname = dname + "/" + os.path.split(dname)[1] + ".shp"
        # don't actually need to compute the centroid because the block group file has it under the INTPTLAT10 and INTPTLON10 cols
        #df = geopandas.read_file(shape_fname, include_fields=["GEOID10", "geometry"])
        #df = df.to_crs(crs=4326)
        #df["centroid"] = df.centroid
        print("Reading shape files at", shape_fname)
        df = geopandas.read_file(shape_fname, include_fields=["GEOID10", "INTPTLAT10", "INTPTLON10"], ignore_geometry=True)
        df.GEOID10 = df.GEOID10.astype("int64")
        df.INTPTLAT10 = df.INTPTLAT10.astype("float32")
        df.INTPTLON10 = df.INTPTLON10.astype("float32")
        df.to_csv(shape_fname + ".csv")
        print("Wrote", len(df.index), "GEOID locations to", shape_fname + ".csv")
        #print(df.dtypes)
        geoid_locs_map.update(df.set_index("GEOID10").T.to_dict("list"))


if __name__ == "__main__":
    t = time.time()
    parser = argparse.ArgumentParser(description="Convert UrbanPop feather files to C++ struct binary file")
    parser.add_argument("--output", "-o", required=True, help="Output file")
    parser.add_argument("--files", "-f", required=True, nargs="+", help="Feather files")
    parser.add_argument("--shape_files_dir", "-s", required=True, nargs="+",
                        help="Directories for census block group shape files. Available from\n" + \
                        "https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2010&layergroup=Block+Groups")
    parser.add_argument("--day_night_files", "-d", required=True, nargs="+",
                        help="Feather files containing daytime and nighttime locations")
    args = parser.parse_args()

    geoid_locs_map = {}
    process_census_bg_shape_file(args.shape_files_dir, geoid_locs_map)
    print("GEOID to locations map contains", len(geoid_locs_map), "entries")

    df_dt_nt = process_nt_dt_feather_files(args.day_night_files)

    process_feather_files(args.files, args.output, geoid_locs_map, df_dt_nt)

    print("Processed", len(args.files), "files in %.2f s" % (time.time() - t))
